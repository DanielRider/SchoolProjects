{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPj3LtNq3fBScymHmGxnuhr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["The homework starts off with my thoughts as I go through the start.  Experiment/functions are towards the end."],"metadata":{"id":"gY4wOB60I37V"}},{"cell_type":"code","execution_count":198,"metadata":{"id":"wmderZ4J-AsA","executionInfo":{"status":"ok","timestamp":1683778023689,"user_tz":300,"elapsed":123,"user":{"displayName":"Daniel Rider","userId":"04019406679242190719"}}},"outputs":[],"source":["import numpy as np\n","d = 400\n","sigma = 1.0\n","r = 12\n","mu = 0.0\n","\n","Centers = [[np.random.default_rng().normal(mu,sigma,d)] for i in range(2)]\n","Centers = np.squeeze(np.asarray(Centers))\n","\n","for i in range(2):\n","  Centers = Centers/np.linalg.norm(Centers[i]) * r\n","\n"]},{"cell_type":"markdown","source":["Generating points using code from last time.  Main difference is multiplying the normed vectors by 12 so that the ball has a radius of 12."],"metadata":{"id":"sXNd3HToDmxO"}},{"cell_type":"code","source":["dist = np.linalg.norm(Centers[0] - Centers[1])\n","print(dist)\n","print(12*np.sqrt(2))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lQuyW59qBs97","executionInfo":{"status":"ok","timestamp":1683778025278,"user_tz":300,"elapsed":120,"user":{"displayName":"Daniel Rider","userId":"04019406679242190719"}},"outputId":"4912bccb-9b8e-4174-9fbc-e1a3ea866e56"},"execution_count":199,"outputs":[{"output_type":"stream","name":"stdout","text":["17.894081612569032\n","16.970562748477143\n"]}]},{"cell_type":"markdown","source":["Validating that the centers are roughly 12sqrt(2) distance away from each other "],"metadata":{"id":"Hdlg1YhGDxGS"}},{"cell_type":"code","source":["n = 80\n","mu1 = Centers[0]\n","mu2 = Centers[1]\n","\n","Gaussian1 = [[np.random.default_rng().normal(mu1,sigma,d)] for i in range(n)]\n","Gaussian2 = [[np.random.default_rng().normal(mu2,sigma,d)] for i in range(n)]\n","Gaussian1 = np.squeeze(np.asarray(Gaussian1))\n","Gaussian2 = np.squeeze(np.asarray(Gaussian2))\n","A = np.concatenate((Gaussian1,Gaussian2),axis=0)\n","print(A)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rwWfjDn-B2X3","executionInfo":{"status":"ok","timestamp":1683778026757,"user_tz":300,"elapsed":117,"user":{"displayName":"Daniel Rider","userId":"04019406679242190719"}},"outputId":"de74f4ff-2714-4220-a31e-299d56eb3252"},"execution_count":200,"outputs":[{"output_type":"stream","name":"stdout","text":["[[-0.11236156  1.78802642  0.35796061 ...  0.53118395 -0.89986854\n","  -0.42414688]\n"," [-0.76677168 -0.34467065  0.65407114 ... -0.33829524 -1.00964968\n","   1.14284742]\n"," [ 0.79764935  0.8472079  -1.5114156  ...  1.99103282 -0.95264174\n","  -0.46384327]\n"," ...\n"," [ 0.64357683 -0.16862523 -1.15835399 ... -0.5283987   0.44253044\n","  -0.32281664]\n"," [ 1.37086906 -0.09203618 -0.56750535 ... -0.08253563  0.52055045\n","   0.78445987]\n"," [-1.27341732  0.46963824  0.20757174 ... -1.88131677 -0.31866876\n","  -0.24686589]]\n"]}]},{"cell_type":"markdown","source":["Just generated two gaussians using the centers on the surface of the r=12 d-dimensional ball we generated earlier.  I concatenated them for the problem but going back isnt too hard from here either.  Now we want to choose a basepoint, and calculate the distance from said points to all other points. I'll just choose the first one and be lazy."],"metadata":{"id":"6aMrberxEykr"}},{"cell_type":"code","source":["basepoint = A[0]\n","distances = []\n","for i in range(2*n):\n","  distances.append(np.linalg.norm(basepoint - A[i]))"],"metadata":{"id":"qxPFyNdWExLc","executionInfo":{"status":"ok","timestamp":1683778028653,"user_tz":300,"elapsed":135,"user":{"displayName":"Daniel Rider","userId":"04019406679242190719"}}},"execution_count":201,"outputs":[]},{"cell_type":"markdown","source":["Now we have a list of length 160 with all the distances.  We want to know the accuracy of our crude clustering algo.  Where the 80 shortest distances belong to the 1st center, and 80 longer distances belong to the 2nd center.  We already know the 80 first distances belong to the first center.  The problem is that merely sorting and separating doesnt really give feedback on accuracy."],"metadata":{"id":"ddXTp57sLPzS"}},{"cell_type":"code","source":["sorted_dist = distances[:]\n","sorted_dist.sort()\n","predGaussian1 = sorted_dist[:n]\n","predGaussian2 = sorted_dist[n+1:]\n","actualGaussian1 = distances[:n]\n","actualGaussian2 = distances[n+1:]\n"],"metadata":{"id":"2smMN_WhGnp8","executionInfo":{"status":"ok","timestamp":1683778030181,"user_tz":300,"elapsed":119,"user":{"displayName":"Daniel Rider","userId":"04019406679242190719"}}},"execution_count":202,"outputs":[]},{"cell_type":"markdown","source":["Basically after sorting we have a predicted cluster of the first 80 in the sorted list.  We know our real cluster is the first 80 in the unsorted list.  \n","So our predGaussian1/2 is our sorted distances meaning we are predicting these distances belong to these gaussians.  ActualGaussian1/2 is our unsorted distances from earlier so we know these are the correct."],"metadata":{"id":"t0SqWSMvt4JI"}},{"cell_type":"code","source":["errors = 0\n","\n","for x in predGaussian2:\n","  if (x not in actualGaussian2):  #Im kind of exploiting/relying on the fact that our distances are going to be pretty unique given we calc up to like 10 decimal points.\n","      errors += 1                  #if a certain distance is in our predicted set but not in our actual set then thats an error.\n","print(errors)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZyxiZ307ITB-","executionInfo":{"status":"ok","timestamp":1683778038307,"user_tz":300,"elapsed":108,"user":{"displayName":"Daniel Rider","userId":"04019406679242190719"}},"outputId":"808cdec4-f1a1-4d8e-a3bd-d5ac4936fcdc"},"execution_count":212,"outputs":[{"output_type":"stream","name":"stdout","text":["1\n"]}]},{"cell_type":"markdown","source":["Surprisingly, there are only 1-3 misclassified data points in the crude clustering algorithm with r = 12.  \n","158/160 = 98.75% accuracy.\n","\n","\n"],"metadata":{"id":"PvUkhl9fQsv-"}},{"cell_type":"markdown","source":["Now we start the SVD"],"metadata":{"id":"sZPyHUok7CTT"}},{"cell_type":"markdown","source":["This was my first attempt at the SVD code.  My finalized code is located towards the bottom under \"SVDCluster\" function.  Basically this first attempt at the SVDcluster doesnt really work."],"metadata":{"id":"WHunn5qsJokr"}},{"cell_type":"code","source":["u,d,vt = np.linalg.svd(A)\n","v = np.transpose(vt)  #this line is a mistake as I learn later on\n","Vk = [[v[i] for i in range(2)]]\n","Vk = np.squeeze(np.asarray(Vk))\n","np.shape(Vk)          #this shape is wrong, the point is to reduce dimensions \n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EpfBS-7Moxyr","executionInfo":{"status":"ok","timestamp":1683776562075,"user_tz":300,"elapsed":152,"user":{"displayName":"Daniel Rider","userId":"04019406679242190719"}},"outputId":"496dd29b-ff23-4f68-968c-4b201d1b3231"},"execution_count":88,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2, 400)"]},"metadata":{},"execution_count":88}]},{"cell_type":"code","source":["Ak = A[:]         \n","for i in range(2*n):\n","  for j in range(2):\n","    Ak[i] = (np.dot(A[i],Vk[j])/np.dot(Vk[j],Vk[j])*Vk[j])\n","np.shape(Ak)\n","# print(A)\n","# print(Ak)"],"metadata":{"id":"VHY_sgGLcaWY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683776841504,"user_tz":300,"elapsed":138,"user":{"displayName":"Daniel Rider","userId":"04019406679242190719"}},"outputId":"9d69bbce-8350-4d70-9392-6c7638466b8b"},"execution_count":99,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(160, 400)"]},"metadata":{},"execution_count":99}]},{"cell_type":"markdown","source":["The following code is just the same clustering algo from earlier but using Ak instead of A"],"metadata":{"id":"el7df9hZ_x5z"}},{"cell_type":"code","source":["basepoint = Ak[0]\n","distances = []\n","n = int(len(A)/2)\n","# print(n)\n","for i in range(n):\n","  distances.append(np.linalg.norm(basepoint - A[i]))\n","\n","sorted_dist = distances[:]\n","sorted_dist.sort()\n","predGaussian1 = sorted_dist[:n]\n","predGaussian2 = sorted_dist[n+1:]\n","actualGaussian1 = distances[:n]\n","actualGaussian2 = distances[n+1:]\n","\n","errors = 0\n","\n","for x in predGaussian2:\n","  if (x not in actualGaussian2):\n","      errors += 1\n","\n","print(errors)\n","print(\"Accuracy is: \" + str((n - errors)/(n)))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e5V-JslK6oFp","executionInfo":{"status":"ok","timestamp":1683777732580,"user_tz":300,"elapsed":118,"user":{"displayName":"Daniel Rider","userId":"04019406679242190719"}},"outputId":"2c8e7626-0cdd-4bdd-b0de-09c73b2b8453"},"execution_count":127,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","Accuracy is: 1.0\n"]}]},{"cell_type":"markdown","source":["100% accuracy on the first try"],"metadata":{"id":"BQiTAR597eAx"}},{"cell_type":"markdown","source":[":I'm actually going to make some functions to make it even nicer.  "],"metadata":{"id":"tMQHTulnYkHl"}},{"cell_type":"code","source":["from typing import List\n","import numpy as np\n","\n","\n","def generateTwoGaussians(d: int, r: int, n: int) -> List[List[float]]:\n","  sigma = 1.0\n","  mu = 0.0\n","  Centers = [[np.random.default_rng().normal(mu,sigma,d)] for i in range(2)]\n","  Centers = np.squeeze(np.asarray(Centers))\n","\n","  for i in range(2):\n","    Centers = Centers/np.linalg.norm(Centers[i]) * r\n","\n","  mu1 = Centers[0]\n","  mu2 = Centers[1]\n","\n","  Gaussian1 = [[np.random.default_rng().normal(mu1,sigma,d)] for i in range(n)]\n","  Gaussian2 = [[np.random.default_rng().normal(mu2,sigma,d)] for i in range(n)]\n","  Gaussian1 = np.squeeze(np.asarray(Gaussian1))\n","  Gaussian2 = np.squeeze(np.asarray(Gaussian2))\n","  A = np.concatenate((Gaussian1,Gaussian2),axis=0)\n","  return A\n","\n","def crudeCluster(A: List[List[float]]) -> float:\n","  basepoint = A[0]\n","  distances = []\n","  n = int(len(A)/2)\n","  # print(\"n is: \" + str(n))\n","  for i in range(2*n):\n","    distances.append(np.linalg.norm(basepoint - A[i]))\n","  # print(\"distances: \" + str(distances))\n","  sorted_dist = distances[:]\n","  sorted_dist.sort()\n","  # print(\"sorted_dist\" + str(sorted_dist))\n","  predGaussian1 = sorted_dist[:n]\n","  predGaussian2 = sorted_dist[n+1:]\n","  actualGaussian1 = distances[:n]\n","  actualGaussian2 = distances[n+1:]\n","  # print(\"predGaussian1: \" + str(predGaussian1))\n","  # print(\"actualGaussian1: \" + str(actualGaussian1))\n","  # print(\"predGaussian2: \" + str(predGaussian2))\n","  # print(\"actualGaussian2: \" + str(actualGaussian2))\n","\n","  errors = 0\n","  for x in predGaussian1:\n","    if (x not in actualGaussian1):\n","        errors += 1\n","\n","  # print(\"Crude errors:\" + str(errors))\n","  # print(\"Crude Accuracy is: \" + str((n - errors)/(n)))\n","  return errors\n","\n","def SVDCluster(A: List[List[float]]) -> float:\n","  u,d,vt = np.linalg.svd(A)\n","  # vt = np.transpose(vt)        #we are given v transpose, take transpose again to get v,  I WAS TAKING THE TRANSPOSE OF THE TRANSPOSE BUT IT WORKS SO MUCH BETTER IF I DONT\n","  Vk = [[vt[i]] for i in range(2)] #take top two right singular vectors\n","  Vk = np.squeeze(np.asarray(Vk)) #I put this code everywhere because im doing something wrong with the matrix/array notation but this fixes it\n","  # print(Vk)\n","  s = (len(A),len(A[0]))        \n","  Ak = np.zeros(s)            #basically I want to start out with an array of 0s and this is how I did it.\n","  # print(Ak)\n","  n = int(len(A)/2)\n","  for i in range(2*n):\n","    for j in range(2):\n","      Ak[i] += (np.dot(A[i],Vk[j])/np.dot(Vk[j],Vk[j]))*Vk[j]\n","  # print(Ak)\n","  basepoint = Ak[0]\n","  distances = []\n","  \n","  for i in range(2*n):\n","    distances.append(np.linalg.norm(basepoint - Ak[i]))\n","\n","  sorted_dist = distances[:]\n","  sorted_dist.sort()\n","\n","  predGaussian1 = sorted_dist[:n]\n","  predGaussian2 = sorted_dist[n+1:]\n","  actualGaussian1 = distances[:n]\n","  actualGaussian2 = distances[n+1:]\n","  # print(\"predGaussian1: \" + str(predGaussian1))\n","  # print(\"actualGaussian1: \" + str(actualGaussian1))\n","  # print(\"predGaussian2: \" + str(predGaussian2))\n","  # print(\"actualGaussian2: \" + str(actualGaussian2))\n","\n","  errors = 0\n","  for x in predGaussian1:\n","    if (x not in actualGaussian1):\n","        errors += 1\n","\n","  # print(\"SVD Cluster errors:\" + str(errors))\n","  # print(\"SVD Accuracy is: \" + str((n - errors)/(n)))\n","  return errors\n"],"metadata":{"id":"_YG0AXiEOgQl","executionInfo":{"status":"ok","timestamp":1683781846746,"user_tz":300,"elapsed":137,"user":{"displayName":"Daniel Rider","userId":"04019406679242190719"}}},"execution_count":408,"outputs":[]},{"cell_type":"code","source":["B = generateTwoGaussians(d = 400,r = 12,n = 80)\n","crudeCluster(B)\n","SVDCluster(B)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rzkj4TW_Zbmm","executionInfo":{"status":"ok","timestamp":1683781826189,"user_tz":300,"elapsed":133,"user":{"displayName":"Daniel Rider","userId":"04019406679242190719"}},"outputId":"c8a61229-089d-418b-fc4d-9a7269e452b8"},"execution_count":406,"outputs":[{"output_type":"stream","name":"stdout","text":["Crude errors:1\n","Crude Accuracy is: 0.9875\n","SVD Cluster errors:0\n","SVD Accuracy is: 1.0\n"]},{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":406}]},{"cell_type":"code","source":["CrudeErrors = 0\n","SVDErrors = 0\n","n = 50*80\n","for i in range(50):\n","  C = generateTwoGaussians(d=400, r = 12, n = 80)\n","  CrudeErrors += crudeCluster(C)\n","  SVDErrors += SVDCluster(C)\n","\n","print(\"d = 400, r = 12, 80 pts per gaussian center\")\n","print(\"Crude errors:\" + str(CrudeErrors))\n","print(\"Crude Accuracy is: \" + str((n - CrudeErrors)/(n)))\n","print(\"SVD Cluster errors:\" + str(SVDErrors))\n","print(\"SVD Accuracy is: \" + str((n - SVDErrors)/(n)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W-29fIdS8QDU","executionInfo":{"status":"ok","timestamp":1683781853420,"user_tz":300,"elapsed":3429,"user":{"displayName":"Daniel Rider","userId":"04019406679242190719"}},"outputId":"b51c5912-9e68-4027-ebc6-46e42f42da88"},"execution_count":409,"outputs":[{"output_type":"stream","name":"stdout","text":["d = 400, r = 12, 80 pts per gaussian center\n","Crude errors:20\n","Crude Accuracy is: 0.995\n","SVD Cluster errors:0\n","SVD Accuracy is: 1.0\n"]}]},{"cell_type":"code","source":["CrudeErrors = 0\n","SVDErrors = 0\n","n = 50*80\n","for i in range(50):\n","  C = generateTwoGaussians(d=400, r = 8, n = 80)\n","  CrudeErrors += crudeCluster(C)\n","  SVDErrors += SVDCluster(C)\n","\n","print(\"d = 400, r = 8, 80 pts per gaussian center\")\n","print(\"Crude errors:\" + str(CrudeErrors))\n","print(\"Crude Accuracy is: \" + str((n - CrudeErrors)/(n)))\n","print(\"SVD Cluster errors:\" + str(SVDErrors))\n","print(\"SVD Accuracy is: \" + str((n - SVDErrors)/(n)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t1qc7nBdMI-z","executionInfo":{"status":"ok","timestamp":1683781862145,"user_tz":300,"elapsed":3030,"user":{"displayName":"Daniel Rider","userId":"04019406679242190719"}},"outputId":"41f603b6-b6ac-4798-b06a-d64b349b4dcc"},"execution_count":410,"outputs":[{"output_type":"stream","name":"stdout","text":["d = 400, r = 8, 80 pts per gaussian center\n","Crude errors:452\n","Crude Accuracy is: 0.887\n","SVD Cluster errors:0\n","SVD Accuracy is: 1.0\n"]}]},{"cell_type":"code","source":["CrudeErrors = 0\n","SVDErrors = 0\n","n = 50*80\n","for i in range(50):\n","  C = generateTwoGaussians(d=400, r = 4.2, n = 80)\n","  CrudeErrors += crudeCluster(C)\n","  SVDErrors += SVDCluster(C)\n","\n","print(\"d = 400, r = 4.2, 80 pts per gaussian center\")\n","print(\"Crude errors:\" + str(CrudeErrors))\n","print(\"Crude Accuracy is: \" + str((n - CrudeErrors)/(n)))\n","print(\"SVD Cluster errors:\" + str(SVDErrors))\n","print(\"SVD Accuracy is: \" + str((n - SVDErrors)/(n)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r21WemVaMUxe","executionInfo":{"status":"ok","timestamp":1683781871782,"user_tz":300,"elapsed":4625,"user":{"displayName":"Daniel Rider","userId":"04019406679242190719"}},"outputId":"4df56794-8fee-4073-b5ca-43adc0c9b098"},"execution_count":411,"outputs":[{"output_type":"stream","name":"stdout","text":["d = 400, r = 4.2, 80 pts per gaussian center\n","Crude errors:1356\n","Crude Accuracy is: 0.661\n","SVD Cluster errors:128\n","SVD Accuracy is: 0.968\n"]}]},{"cell_type":"code","source":["CrudeErrors = 0\n","SVDErrors = 0\n","n = 50*80\n","for i in range(50):\n","  C = generateTwoGaussians(d=6, r = 8, n = 80)\n","  CrudeErrors += crudeCluster(C)\n","  SVDErrors += SVDCluster(C)\n","\n","print(\"d = 6, r = 8, 80 pts per gaussian center\")\n","print(\"Crude errors:\" + str(CrudeErrors))\n","print(\"Crude Accuracy is: \" + str((n - CrudeErrors)/(n)))\n","print(\"SVD Cluster errors:\" + str(SVDErrors))\n","print(\"SVD Accuracy is: \" + str((n - SVDErrors)/(n)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pVkxAtBLMjaM","executionInfo":{"status":"ok","timestamp":1683781885207,"user_tz":300,"elapsed":1336,"user":{"displayName":"Daniel Rider","userId":"04019406679242190719"}},"outputId":"f666981e-31b3-4197-f362-3bf78d5454ff"},"execution_count":413,"outputs":[{"output_type":"stream","name":"stdout","text":["d = 6, r = 8, 80 pts per gaussian center\n","Crude errors:10\n","Crude Accuracy is: 0.9975\n","SVD Cluster errors:1\n","SVD Accuracy is: 0.99975\n"]}]},{"cell_type":"markdown","source":["SVD might be too good... I might be cheating\n","\n","I was cheating a little.  But I fixed it.\n","\n","Thoughts uhh pretty interesting.  I'm still a little bit worried that I didn't implement the SVD correctly, but the fact that it performs poorly on r = 4.2 distance gaussians gives me hope. If I am doing it correctly, then its pretty incredible how well it works."],"metadata":{"id":"H8U0KR2-M1sP"}},{"cell_type":"markdown","source":["# In this homework, you'll implement the strategy for separating Gaussians described in 3.9.3 \"Clustering a Mixture of Spherical Gaussians\" (and also relevant is 2.8 \"Separating Gaussians\").  The idea is that a crude clustering algorithm plus SVD yields a better clustering algorithm.\n","\n","Generating Data Points\n","\n","Working in 400-dimensional space, first generate 2 random points \n","μ\n","1\n"," and \n","μ\n","2\n"," on the surface of the sphere centered at the origin with radius r=12.  (Note that in 400-dimensional space, we expect the two vectors two be nearly orthogonal, so the distance between the two points should be around \n","12\n","2\n",".  You might want to check that this is so for your two points.)  These will be the centers of our Gaussian distributions.  Generate 80 random points from a Gaussian distribution with center \n","μ\n","1\n",", and then another 80 points from a Gaussian distribution with center \n","μ\n","2\n",".  For all Gaussian distrubutions, we'll just use \n","σ\n","=\n","1\n",".\n","\n","The Crude Clustering Algorithm\n","\n","The crude algorithm assumes in advance that there are the same number of points (80 of them) generated from each Gaussian, but doesn't know which is which. \n","\n","Pick one data point as a basepoint.  You could pick randomly, or just always use the first data point generated.  Or, you could slightly reduce the chances to choose a weird outlier as your basepoint as follows:  Compute the distances between all pairs of points from your set of 160 and find the minimum.  Choose as your basepoint one of the points that has minimum distance to another point.\n","Make a list of the distances from your basepoint to all 160 points in the set. \n","Take the 80 points (including the basepoint) whose distances to the basepoint are smallest, and say that these are probably from the same distribution as your basepoint.\n","You should find that this crude method works pretty well if you generate data as described above, but works less will if you decrease r to 8 or 4. \n","\n","Optional: Perhaps Somewhat Less Crude Clustering Algorithm\n","\n","Instead of assuming there are 80 points from each distribution, you could in step 3 use a better method that looks for a cluster of the smallest points in your list, looking for a gap between the small values and the large values.  We haven't discussed such methods in this class, but maybe you know one or would like to find one some other way. \n","\n","If you use another method, your method should only use the list of distances from the basepoint, and not any other information about the positions of the data points.  Of course there are many methods of clustering the original data set, but if you use some other method, it probably sort of defeats the purpose of this exercise.  If you do use a method that doesn't assume the same number of points from each Gaussian, then instead of generating 80 points from each Gaussian, you could randomly generate 60-100 points from each.\n","\n","Clustering Using SVD\n","\n","Find the best fit 2-dimensional subspace K to your 160 data points.  To do this, use the SVD and take the top 2 right singular vectors as a basis for K.  (Don't write your own code to find the singular vectors.  Let Matlab or numpy find the SVD for you.)\n","Compute the orthogonal projections of your data points onto K.\n","Use your crude clustering algorithm on the set of projections.\n","The Experiment (or at least \"playing around to see how well these work\")\n","\n","Generate data as described in \"Generating Data Points\".  Use the Crude Clustering Algorithm to try to separate the points.  Compute what percentage of the points were classified correctly (or do the percentage classified incorrectly, if you prefer that point of view).  Then try the separation via the Clustering Using SVD algorithm, and again compute the percentage classified correctly.  Repeat 50 times and report the average percentages over the 50 repetitions.\n","\n","Then do it all again, using r=8 and r=4.2 instead of r=12.  (Recall r is the distance from the origin to each Gaussian center.)  Also try it using r=8 again, but in 6-dimensional space instead of d=400.\n","\n","Comment on the results.   Submit: (1) your code (in a text file or some format that someone could actually try to run---not a pdf of a screenshot), (2) some sample output, and (3) your comments.\n","\n","Bonus Complication\n","\n","If you want extra points, you can do this all with 4 gaussian distributions instead of just 2.  You'd have a set of 320 points instead of 160. \n","\n","The crude algorithm would choose a basepoint and then select the 80 nearest points for one cluster.  Remove these from the list, choose a new basepoint, and again select the 80 nearest points for the next cluster.  Remove these from the list, and then repeat again to separate the remaining 160 points.\n","The SVD algorithm would use the best-fit 4-dimensional subspace instead of the best-fit 2-dimensional subspace. \n"," "],"metadata":{"id":"xhCcAx8h-Esa"}},{"cell_type":"markdown","source":[],"metadata":{"id":"vYA_99mgI1XL"}}]}